{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edb69852",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02e46404",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a11cae1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "# setMaster() - Set Spark Content Manager which is local[cpu_cores] \n",
    "config = SparkConf().setMaster(\"local[4]\").setAppName(\"ETL Pipeline\")\n",
    "sc = SparkContext(conf=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2bec01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"ETL Pipeline\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef116815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.8</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ETL Pipeline</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f79eb7ef898>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc78d0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hremployeeDF = spark.read.format(\"jdbc\")\\\n",
    ".option(\"url\", \"jdbc:mysql://localhost:3306/hremployeeDB\")\\\n",
    ".option(\"dbtable\", \"HR_Employee\").option(\"user\",\"root\").option(\"password\",\"hadoop@123\")\\\n",
    ".option(\"driver\", \"com.mysql.cj.jdbc.Driver\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad693449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------------+---------+------+---+-------------+-------------+--------------+-----------------+--------------+--------+---------------+----------+------+----------+--------+------+-----------------------+---------------+---------------------+---------------+------------------+\n",
      "|EmployeeID|          Department|           JobRole|Attrition|Gender|Age|MaritalStatus|    Education|EducationField|   BusinessTravel|JobInvolvement|JobLevel|JobSatisfaction|Hourlyrate|Income|Salaryhike|OverTime|Workex|YearsSinceLastPromotion|EmpSatisfaction|TrainingTimesLastYear|WorkLifeBalance|Performance_Rating|\n",
      "+----------+--------------------+------------------+---------+------+---+-------------+-------------+--------------+-----------------+--------------+--------+---------------+----------+------+----------+--------+------+-----------------------+---------------+---------------------+---------------+------------------+\n",
      "|\n",
      "|\n",
      "+----------+--------------------+------------------+---------+------+---+-------------+-------------+--------------+-----------------+--------------+--------+---------------+----------+------+----------+--------+------+-----------------------+---------------+---------------------+---------------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hremployeeDF.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "448428a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Scan JDBCRelation(HR_Employee) [numPartitions=1] [EmployeeID#0,Department#1,JobRole#2,Attrition#3,Gender#4,Age#5,MaritalStatus#6,Education#7,EducationField#8,BusinessTravel#9,JobInvolvement#10,JobLevel#11,JobSatisfaction#12,Hourlyrate#13,Income#14,Salaryhike#15,OverTime#16,Workex#17,YearsSinceLastPromotion#18,EmpSatisfaction#19,TrainingTimesLastYear#20,WorkLifeBalance#21,Performance_Rating#22] PushedFilters: [], ReadSchema: struct<EmployeeID:int,Department:string,JobRole:string,Attrition:string,Gender:string,Age:int,Mar...\n"
     ]
    }
   ],
   "source": [
    "# Show Physical plan of execution which is known as DAG.\n",
    "hremployeeDF.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cada100d",
   "metadata": {},
   "source": [
    "#### Materialized View of Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a34ed488",
   "metadata": {},
   "outputs": [],
   "source": [
    "hremployeeDF.createOrReplaceTempView(\"hremployee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1b7da2",
   "metadata": {},
   "source": [
    "#### 1. Display Shape of hremployee table \n",
    "    * Show number of rows & number of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "87ad80fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+\n",
      "|row_count|columns_count|\n",
      "+---------+-------------+\n",
      "|     1469|           23|\n",
      "+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "select count(*)  as row_count, {num_of_cols} as columns_count from hremployee\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0a0f30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_cols = len(hremployeeDF.columns)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "df8c247e",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "select count(*) from information_schema.columns WHERE table_name = 'HR_Employee';\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5217ec3e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------+\n",
      "|            col_name|data_type|comment|\n",
      "+--------------------+---------+-------+\n",
      "|          EmployeeID|      int|   null|\n",
      "|          Department|   string|   null|\n",
      "|             JobRole|   string|   null|\n",
      "|           Attrition|   string|   null|\n",
      "|              Gender|   string|   null|\n",
      "|                 Age|      int|   null|\n",
      "|       MaritalStatus|   string|   null|\n",
      "|           Education|   string|   null|\n",
      "|      EducationField|   string|   null|\n",
      "|      BusinessTravel|   string|   null|\n",
      "|      JobInvolvement|   string|   null|\n",
      "|            JobLevel|      int|   null|\n",
      "|     JobSatisfaction|   string|   null|\n",
      "|          Hourlyrate|      int|   null|\n",
      "|              Income|      int|   null|\n",
      "|          Salaryhike|      int|   null|\n",
      "|            OverTime|   string|   null|\n",
      "|              Workex|      int|   null|\n",
      "|YearsSinceLastPro...|      int|   null|\n",
      "|     EmpSatisfaction|   string|   null|\n",
      "+--------------------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe hremployee\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e179d6cf",
   "metadata": {},
   "source": [
    "#### 2. Write a Query to show first three employee from each Job Role to join the company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "692135e4",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+-------+\n",
      "|EmployeeID|          Department|             JobRole|row_num|\n",
      "+----------+--------------------+--------------------+-------+\n",
      "|         1|               Sales|     Sales Executive|      1|\n",
      "|        28|               Sales|     Sales Executive|      2|\n",
      "|        40|               Sales|     Sales Executive|      3|\n",
      "|         9|Research & Develo...|Manufacturing Dir...|      1|\n",
      "|        16|Research & Develo...|Manufacturing Dir...|      2|\n",
      "|        21|Research & Develo...|Manufacturing Dir...|      3|\n",
      "|         3|Research & Develo...|Laboratory Techni...|      1|\n",
      "|         5|Research & Develo...|Laboratory Techni...|      2|\n",
      "|         6|Research & Develo...|Laboratory Techni...|      3|\n",
      "|        22|               Sales|Sales Representative|      1|\n",
      "|        34|               Sales|Sales Representative|      2|\n",
      "|        37|               Sales|Sales Representative|      3|\n",
      "|        10|Research & Develo...|Healthcare Repres...|      1|\n",
      "|        29|Research & Develo...|Healthcare Repres...|      2|\n",
      "|        32|Research & Develo...|Healthcare Repres...|      3|\n",
      "|         2|Research & Develo...|  Research Scientist|      1|\n",
      "|         4|Research & Develo...|  Research Scientist|      2|\n",
      "|        13|Research & Develo...|  Research Scientist|      3|\n",
      "|        19|               Sales|             Manager|      1|\n",
      "|        26|Research & Develo...|             Manager|      2|\n",
      "|        30|               Sales|             Manager|      3|\n",
      "|        23|Research & Develo...|   Research Director|      1|\n",
      "|        46|Research & Develo...|   Research Director|      2|\n",
      "|        56|Research & Develo...|   Research Director|      3|\n",
      "|        80|     Human Resources|     Human Resources|      1|\n",
      "|       101|     Human Resources|     Human Resources|      2|\n",
      "|       135|     Human Resources|     Human Resources|      3|\n",
      "+----------+--------------------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" select * from (\n",
    "select EmployeeID, Department, JobRole, row_number() \n",
    "over(partition by JobRole order by EmployeeID) as row_num from hremployee)\n",
    "where row_num < 4\"\"\").show(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebf60d6",
   "metadata": {},
   "source": [
    "#### 3. Write a Query to show Top three Employee from each JobRole earning Highest Salary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f397bae6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "190944fa",
   "metadata": {},
   "source": [
    "#### 4. Show Top 3 Highest Package from overall Job Role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f148a5e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d46f1299",
   "metadata": {},
   "source": [
    "#### 5. Lag()\n",
    "Write a Spark SQL query to show employee in Ascending Order with respect to employee income comapred to previous income for each job role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9a67fd0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------+-----------+----------+\n",
      "|EmployeeID|             JobRole|Income|prev_income|difference|\n",
      "+----------+--------------------+------+-----------+----------+\n",
      "|        10|Healthcare Repres...|  5237|       null|      null|\n",
      "|       285|Healthcare Repres...|  4741|      13496|     -8755|\n",
      "|      1183|Healthcare Repres...|  6842|      13966|     -7124|\n",
      "|      1157|Healthcare Repres...|  4148|      11245|     -7097|\n",
      "|       205|Healthcare Repres...|  6673|      13734|     -7061|\n",
      "|       677|Healthcare Repres...|  4014|      10552|     -6538|\n",
      "|       397|Healthcare Repres...|  4522|      10965|     -6443|\n",
      "|       833|Healthcare Repres...|  5731|      12169|     -6438|\n",
      "|      1065|Healthcare Repres...|  4035|      10466|     -6431|\n",
      "|       745|Healthcare Repres...|  4777|      10999|     -6222|\n",
      "|       736|Healthcare Repres...|  4240|      10388|     -6148|\n",
      "|      1098|Healthcare Repres...|  4069|      10124|     -6055|\n",
      "|        89|Healthcare Repres...|  4152|      10096|     -5944|\n",
      "|       489|Healthcare Repres...|  4089|       9824|     -5735|\n",
      "|       929|Healthcare Repres...|  7978|      13577|     -5599|\n",
      "|       105|Healthcare Repres...|  5163|      10673|     -5510|\n",
      "|       267|Healthcare Repres...|  5582|      10938|     -5356|\n",
      "|      1231|Healthcare Repres...|  5562|      10748|     -5186|\n",
      "|       555|Healthcare Repres...|  6811|      11103|     -4292|\n",
      "|      1045|Healthcare Repres...|  6651|      10851|     -4200|\n",
      "+----------+--------------------+------+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select *, (Income - prev_income) as difference from (select EmployeeID, JobRole, Income, \n",
    "lag(Income, 1) over (partition by JobRole order by EmployeeID) as prev_income \n",
    "from hremployee) as _ order by JobRole, difference\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2912f276",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+------+----------+\n",
      "|EmployeeID|        JobRole|Income|difference|\n",
      "+----------+---------------+------+----------+\n",
      "|         1|Sales Executive|  5993|      null|\n",
      "|        28|Sales Executive|  6825|       832|\n",
      "|        40|Sales Executive|  5376|     -1449|\n",
      "|        44|Sales Executive|  8726|      3350|\n",
      "|        47|Sales Executive|  4568|     -4158|\n",
      "|        49|Sales Executive|  5772|      1204|\n",
      "|        53|Sales Executive|  5454|      -318|\n",
      "|        55|Sales Executive|  4157|     -1297|\n",
      "|        57|Sales Executive|  9069|      4912|\n",
      "|        64|Sales Executive|  7637|     -1432|\n",
      "|        71|Sales Executive|  5473|     -2164|\n",
      "|        77|Sales Executive|  4312|     -1161|\n",
      "|        83|Sales Executive| 10239|      5927|\n",
      "|        90|Sales Executive|  9619|      -620|\n",
      "|        92|Sales Executive|  5441|     -4178|\n",
      "|        93|Sales Executive|  5209|      -232|\n",
      "|        95|Sales Executive|  5010|      -199|\n",
      "|        97|Sales Executive|  4999|       -11|\n",
      "|        98|Sales Executive|  4221|      -778|\n",
      "|        99|Sales Executive| 13872|      9651|\n",
      "+----------+---------------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select EmployeeID, JobRole, Income, \n",
    "Income - lag(Income, 1) over (partition by JobRole order by EmployeeID) as difference\n",
    "from hremployee\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9938034c",
   "metadata": {},
   "source": [
    "#### Lead()\n",
    "    \n",
    "    * Row's Next Records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fbbffb18",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+---+------+------+------+-----------+\n",
      "|employeeid|          department|             jobrole|age|gender|income|workex|next_income|\n",
      "+----------+--------------------+--------------------+---+------+------+------+-----------+\n",
      "|         1|               Sales|     Sales Executive| 41|Female|  5993|     8|       2090|\n",
      "|         2|Research & Develo...|  Research Scientist| 49|  Male|  5130|    10|       2909|\n",
      "|         3|Research & Develo...|Laboratory Techni...| 37|  Male|  2090|     7|       3468|\n",
      "|         4|Research & Develo...|  Research Scientist| 33|Female|  2909|     8|       3068|\n",
      "|         5|Research & Develo...|Laboratory Techni...| 27|  Male|  3468|     6|       2670|\n",
      "|         6|Research & Develo...|Laboratory Techni...| 32|  Male|  3068|     8|       2693|\n",
      "|         7|Research & Develo...|Laboratory Techni...| 59|Female|  2670|    12|       9526|\n",
      "|         8|Research & Develo...|Laboratory Techni...| 30|  Male|  2693|     1|       5237|\n",
      "|         9|Research & Develo...|Manufacturing Dir...| 38|  Male|  9526|    10|       2426|\n",
      "|        10|Research & Develo...|Healthcare Repres...| 36|  Male|  5237|    17|       4193|\n",
      "|        11|Research & Develo...|Laboratory Techni...| 35|  Male|  2426|     6|       2911|\n",
      "|        12|Research & Develo...|Laboratory Techni...| 29|Female|  4193|    10|       2661|\n",
      "|        13|Research & Develo...|  Research Scientist| 31|  Male|  2911|     5|       2028|\n",
      "|        14|Research & Develo...|Laboratory Techni...| 34|  Male|  2661|     3|       9980|\n",
      "|        15|Research & Develo...|Laboratory Techni...| 28|  Male|  2028|     6|       3298|\n",
      "|        16|Research & Develo...|Manufacturing Dir...| 29|Female|  9980|    10|       2935|\n",
      "|        17|Research & Develo...|  Research Scientist| 32|  Male|  3298|     7|      15427|\n",
      "|        18|Research & Develo...|Laboratory Techni...| 22|  Male|  2935|     1|       3944|\n",
      "|        19|               Sales|             Manager| 53|Female| 15427|    31|       4011|\n",
      "|        20|Research & Develo...|  Research Scientist| 38|  Male|  3944|     6|       3407|\n",
      "+----------+--------------------+--------------------+---+------+------+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select employeeid, department, jobrole, age, gender, income, workex, \n",
    "LEAD(income, 2) over(order by employeeid) as next_income\n",
    "from hremployee\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d41a7e",
   "metadata": {},
   "source": [
    "#### NTILE()\n",
    "    * Dividing Records into percetiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7fe55d8c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+---+------+------+----------------+\n",
      "|employeeid|          department|             jobrole|age|gender|income|salary_quartiles|\n",
      "+----------+--------------------+--------------------+---+------+------+----------------+\n",
      "|       514|Research & Develo...|  Research Scientist| 20|  Male|  1009|               1|\n",
      "|       728|Research & Develo...|  Research Scientist| 18|  Male|  1051|               1|\n",
      "|       765|               Sales|Sales Representative| 28|  Male|  1052|               1|\n",
      "|      1338|               Sales|Sales Representative| 30|  Male|  1081|               1|\n",
      "|      1365|               Sales|Sales Representative| 29|  Male|  1091|               1|\n",
      "|       178|Research & Develo...|Laboratory Techni...| 19|  Male|  1102|               1|\n",
      "|       912|               Sales|Sales Representative| 25|  Male|  1118|               1|\n",
      "|      1402|Research & Develo...|Laboratory Techni...| 31|Female|  1129|               1|\n",
      "|       302|               Sales|Sales Representative| 18|Female|  1200|               1|\n",
      "|       911|Research & Develo...|  Research Scientist| 23|  Male|  1223|               1|\n",
      "|        24|Research & Develo...|  Research Scientist| 21|  Male|  1232|               1|\n",
      "|      1017|Research & Develo...|  Research Scientist| 31|Female|  1261|               1|\n",
      "|      1053|Research & Develo...|  Research Scientist| 30|  Male|  1274|               1|\n",
      "|       516|Research & Develo...|Laboratory Techni...| 35|  Male|  1281|               1|\n",
      "|      1013|               Sales|Sales Representative| 31|Female|  1359|               1|\n",
      "|      1205|Research & Develo...|Laboratory Techni...| 32|  Male|  1393|               1|\n",
      "|       778|Research & Develo...|Laboratory Techni...| 21|Female|  1416|               1|\n",
      "|       297|Research & Develo...|Laboratory Techni...| 18|  Male|  1420|               1|\n",
      "|       150|Research & Develo...|Laboratory Techni...| 19|Female|  1483|               1|\n",
      "|      1311|Research & Develo...|  Research Scientist| 18|Female|  1514|               1|\n",
      "+----------+--------------------+--------------------+---+------+------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select employeeid, department, jobrole, age, gender, \n",
    "income, NTILE(4) over(order by income) as salary_quartiles \n",
    "from hremployee\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8c3427",
   "metadata": {},
   "source": [
    "#### Find the Number of Employee in each percentile_group 0-25th, 25-50th, 50-75th, 75-100th using percent_rank and create category percentile_group using case when."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "63fbc08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+\n",
      "|percentile_group|employee_count|\n",
      "+----------------+--------------+\n",
      "|          0-25th|           367|\n",
      "|         25-50th|           366|\n",
      "|         50-75th|           367|\n",
      "|      75th-100th|           369|\n",
      "+----------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select \n",
    "case \n",
    "    when percetile_rank < 0.25 then \"0-25th\"\n",
    "    when percetile_rank < 0.5 then \"25-50th\"\n",
    "    when percetile_rank < 0.75 then \"50-75th\"\n",
    "    else '75th-100th'\n",
    "END AS percentile_group,\n",
    "count(*) as employee_count\n",
    "from(\n",
    "select employeeid, department,jobrole, age, gender, \n",
    "income, percent_rank() over(partition by department \n",
    "order by income) as percetile_rank \n",
    "from hremployee) \n",
    "group by percentile_group \n",
    "order by percentile_group\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1c502f",
   "metadata": {},
   "source": [
    "#### Hive Integration with PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d498335b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44bb435d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14433 ResourceManager\n",
      "14274 SecondaryNameNode\n",
      "14597 NodeManager\n",
      "13848 NameNode\n",
      "5385 Jps\n",
      "14042 DataNode\n",
      "5306 SparkSubmit\n"
     ]
    }
   ],
   "source": [
    "!jps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a24c787d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark integration with Hive WareHouse.\n",
    "# config for Hive-Integration property-name \"spark.sql.warehouse.dir\" \n",
    "# value = \"/user/hive/warehouse\"\n",
    "spark = (SparkSession.builder.appName(\"pyspark-Hive-Integration\")\n",
    "        .config(\"spark.sql.warehouse.dir\",\"/user/hive/warehouse\")\n",
    "        .enableHiveSupport().getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c2f05be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.8</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-Hive-Integration</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f185e390320>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bee8404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|     default|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05832207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "create database if not exists airlines\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fb82a8d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|    airlines|\n",
      "|     default|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"show databases\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92d4be2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"use airlines\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65eaa9b0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"show tables\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a263e2f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "create table if not exists flights(DayofMonth int, \n",
    "DayOfWeek int, Carrier varchar(10), OriginAirportID int,\n",
    "DestAirportID int, DepDelay int, ArrDelay int)\n",
    "row format delimited\n",
    "fields terminated by ','\n",
    "lines terminated by '\\n'\n",
    "STORED AS TEXTFILE\n",
    "TBLPROPERTIES('skip.header.line.count'='1')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ae6af6a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "|airlines|  flights|      false|\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07a23495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"load data local inpath '/home/hadoop/Downloads/raw_flight_data1.csv'\n",
    "overwrite into table flights\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "043e37f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "create table if not exists airports(airport_id int, \n",
    "city varchar(50), state varchar(50), name varchar(100))\n",
    "row format delimited\n",
    "fields terminated by ','\n",
    "lines terminated by '\\n'\n",
    "STORED AS TEXTFILE\n",
    "TBLPROPERTIES('skip.header.line.count'='1')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8bbd76c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"load data local inpath '/home/hadoop/Downloads/airports1.csv'\n",
    "overwrite into table airports\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2b87663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "|airlines| airports|      false|\n",
      "|airlines|  flights|      false|\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f0c445a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----+--------------------+\n",
      "|airport_id|       city|state|                name|\n",
      "+----------+-----------+-----+--------------------+\n",
      "|     10165|Adak Island|   AK|                Adak|\n",
      "|     10299|  Anchorage|   AK|Ted Stevens Ancho...|\n",
      "|     10304|      Aniak|   AK|       Aniak Airport|\n",
      "|     10754|     Barrow|   AK|Wiley Post/Will R...|\n",
      "|     10551|     Bethel|   AK|      Bethel Airport|\n",
      "|     10926|    Cordova|   AK|Merle K Mudhole S...|\n",
      "|     14709|  Deadhorse|   AK|   Deadhorse Airport|\n",
      "|     11336| Dillingham|   AK|  Dillingham Airport|\n",
      "|     11630|  Fairbanks|   AK|Fairbanks Interna...|\n",
      "|     11997|   Gustavus|   AK|    Gustavus Airport|\n",
      "|     12523|     Juneau|   AK|Juneau International|\n",
      "|     12819|  Ketchikan|   AK|Ketchikan Interna...|\n",
      "|     10245|King Salmon|   AK| King Salmon Airport|\n",
      "|     10170|     Kodiak|   AK|      Kodiak Airport|\n",
      "|     13970|   Kotzebue|   AK| Ralph Wien Memorial|\n",
      "|     13873|       Nome|   AK|        Nome Airport|\n",
      "|     14256| Petersburg|   AK|Petersburg James ...|\n",
      "|     14828|      Sitka|   AK|Sitka Rocky Gutie...|\n",
      "|     12807| St. Mary's|   AK|  St. Mary's Airport|\n",
      "|     11445|   Unalaska|   AK|    Unalaska Airport|\n",
      "+----------+-----------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from airports\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7721f182",
   "metadata": {},
   "source": [
    "#### 1. Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a854aea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_df = spark.table(\"airlines.flights\")\n",
    "airports_df = spark.table(\"airlines.airports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7072afbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----+--------------------+\n",
      "|airport_id|       city|state|                name|\n",
      "+----------+-----------+-----+--------------------+\n",
      "|     10165|Adak Island|   AK|                Adak|\n",
      "|     10299|  Anchorage|   AK|Ted Stevens Ancho...|\n",
      "+----------+-----------+-----+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airports_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db54e3d",
   "metadata": {},
   "source": [
    "### 2. Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff531ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_join = flights_df.join(airports_df, \n",
    "                   on = flights_df.OriginAirportID == airports_df.airport_id, how = \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2873f1c7",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+---------------+-------------+--------+--------+----------+--------------+-----+--------------------+\n",
      "|DayofMonth|DayOfWeek|Carrier|OriginAirportID|DestAirportID|DepDelay|ArrDelay|airport_id|          city|state|                name|\n",
      "+----------+---------+-------+---------------+-------------+--------+--------+----------+--------------+-----+--------------------+\n",
      "|        19|        5|     DL|          11433|        13303|      -3|       1|     11433|       Detroit|   MI|Detroit Metro Way...|\n",
      "|        19|        5|     DL|          14869|        12478|       0|      -8|     14869|Salt Lake City|   UT|Salt Lake City In...|\n",
      "|        19|        5|     DL|          14057|        14869|      -4|     -15|     14057|      Portland|   OR|Portland Internat...|\n",
      "|        19|        5|     DL|          15016|        11433|      28|      24|     15016|     St. Louis|   MO|Lambert-St. Louis...|\n",
      "|        19|        5|     DL|          11193|        12892|      -6|     -11|     11193|    Cincinnati|   OH|Cincinnati/Northe...|\n",
      "+----------+---------+-------+---------------+-------------+--------+--------+----------+--------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_join.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ab2a4c",
   "metadata": {},
   "source": [
    "#### 3. Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8215afe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_join = flights_join.repartition(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3ee98eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_join.write.parquet(\"file:///home/hadoop/Downloads/flights/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7e7a03b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a parquet file format\n",
    "flights_parquet_df = spark.read.parquet(\"file:///home/hadoop/Downloads/flights/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a5706e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_join.write.parquet(\"/flights1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5dae1b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_join.write.partitionBy(\"Carrier\").parquet(\"/airlines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "973d387e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+---------------+-------------+--------+--------+----------+--------------+-----+--------------------+\n",
      "|DayofMonth|DayOfWeek|Carrier|OriginAirportID|DestAirportID|DepDelay|ArrDelay|airport_id|          city|state|                name|\n",
      "+----------+---------+-------+---------------+-------------+--------+--------+----------+--------------+-----+--------------------+\n",
      "|         4|        7|     UA|          13930|        12953|     115|      91|     13930|       Chicago|   IL|Chicago O'Hare In...|\n",
      "|        28|        7|     AS|          14679|        13830|     -11|     -27|     14679|     San Diego|   CA|San Diego Interna...|\n",
      "|         3|        4|     WN|          11259|        12191|      28|      29|     11259|        Dallas|   TX|   Dallas Love Field|\n",
      "|        29|        6|     WN|          11292|        13204|       6|     -11|     11292|        Denver|   CO|Denver International|\n",
      "|         1|        3|     B6|          13204|        12478|      -3|      -1|     13204|       Orlando|   FL|Orlando Internati...|\n",
      "|         3|        6|     DL|          14831|        13487|      -1|       6|     14831|      San Jose|   CA|Norman Y. Mineta ...|\n",
      "|        22|        3|     UA|          10423|        12266|      -3|     -17|     10423|        Austin|   TX|Austin - Bergstro...|\n",
      "|         2|        1|     DL|          14831|        10397|      -5|      -4|     14831|      San Jose|   CA|Norman Y. Mineta ...|\n",
      "|        28|        5|     B6|          12953|        11697|     267|     256|     12953|      New York|   NY|           LaGuardia|\n",
      "|        30|        1|     WN|          14893|        12892|       3|      -6|     14893|    Sacramento|   CA|Sacramento Intern...|\n",
      "|         5|        5|     WN|          15304|        11292|       1|      -2|     15304|         Tampa|   FL| Tampa International|\n",
      "|        25|        3|     AA|          14747|        13930|      -3|      -6|     14747|       Seattle|   WA|Seattle/Tacoma In...|\n",
      "|         7|        7|     DL|          10397|        11697|       3|       1|     10397|       Atlanta|   GA|Hartsfield-Jackso...|\n",
      "|        28|        7|     US|          14492|        11057|      -5|      -6|     14492|Raleigh/Durham|   NC|Raleigh-Durham In...|\n",
      "|        13|        6|     WN|          13871|        12889|      -3|      -9|     13871|         Omaha|   NE|     Eppley Airfield|\n",
      "|         8|        3|     WN|          14107|        12892|      12|      -1|     14107|       Phoenix|   AZ|Phoenix Sky Harbo...|\n",
      "|         9|        7|     EV|          14122|        12266|       8|       2|     14122|    Pittsburgh|   PA|Pittsburgh Intern...|\n",
      "|        10|        1|     AA|          13930|        14747|      -5|     -16|     13930|       Chicago|   IL|Chicago O'Hare In...|\n",
      "|        15|        4|     UA|          12264|        13204|       4|      19|     12264|    Washington|   DC|Washington Dulles...|\n",
      "|        18|        3|     B6|          12191|        10721|      -6|     -18|     12191|       Houston|   TX|     William P Hobby|\n",
      "+----------+---------+-------+---------------+-------------+--------+--------+----------+--------------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_join.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a88b5d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_join.write.bucketBy(col = 'state', numBuckets = 50).format(\"csv\")\\\n",
    ".saveAsTable(\"bucketed_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f2e3eff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_join.write.partitionBy('Carrier').bucketBy(col = 'state', numBuckets = 30)\\\n",
    ".format(\"parquet\").saveAsTable(\"part_bucket_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "be1e7c52",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|carrier|count(1)|\n",
      "+-------+--------+\n",
      "|     UA|  287601|\n",
      "|     AA|  291771|\n",
      "|     EV|  158253|\n",
      "|     B6|  122297|\n",
      "|     DL|  385040|\n",
      "|     OO|  161102|\n",
      "|     F9|   35821|\n",
      "|     YV|   53022|\n",
      "|     US|  235031|\n",
      "|     MQ|  113634|\n",
      "|     HA|   18658|\n",
      "|     AS|   69056|\n",
      "|     FL|   93013|\n",
      "|     VX|   34869|\n",
      "|     WN|  580029|\n",
      "|     9E|   80221|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select carrier, count(*) from part_bucket_table group by carrier\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b57867",
   "metadata": {},
   "source": [
    "#### Load on MySQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a160224e",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_properties = {\n",
    "    'user':\"root\",\n",
    "    'password':\"hadoop@123\",\n",
    "    'driver':'com.mysql.cj.jdbc.Driver'\n",
    "}\n",
    "\n",
    "\n",
    "flights_join.write.jdbc(url=\"jdbc:mysql://localhost:3306/flights\", table= \"airlines\",\n",
    "                       mode=\"overwrite\", properties=connection_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c1e9de77",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "128c333e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483065d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
